# Copy this file to .env and add your actual Claude API key
CLAUDE_API_KEY=sk-ant-your-api-key-here

# Directories
TRANSCRIPTIONS_DIR=./transcriptions
PROMPTS_DIR=./prompts

# Server Configuration
PORT=3000

# Claude Model Selection
# Choose the model to use for transcription formatting and prompt execution
#
# Available Models:
#
# HAIKU (Fast & Cost-Effective):
#   - claude-3-5-haiku-20241022    [RECOMMENDED] Latest Haiku, fastest responses, lowest cost
#   - claude-3-haiku-20240307      Previous Haiku version
#
# SONNET (Balanced Performance):
#   - claude-sonnet-4-5-20250514   Latest Sonnet 4.5, state-of-the-art performance
#   - claude-sonnet-4-20250514     Sonnet 4, excellent quality
#   - claude-3-5-sonnet-20241022   Sonnet 3.5, excellent quality and speed
#   - claude-3-5-sonnet-20240620   Previous Sonnet 3.5 version
#   - claude-3-sonnet-20240229     Sonnet 3 version
#
# OPUS (Highest Intelligence - Legacy):
#   - claude-3-opus-20240229       Most capable legacy model, slower and more expensive
#
# Recommendations:
#   - For transcription formatting: Haiku (fast, accurate, cost-effective)
#   - For complex analysis: Sonnet 4.5 or Sonnet 4 (best quality)
#   - For most tasks: Latest Haiku provides excellent results
#   - For demanding analytical work: Sonnet 4.5 or Opus
#
CLAUDE_MODEL=claude-3-5-haiku-20241022

# System Prompts
# These are the prompt files used internally for transcription formatting
# Files must exist in the prompts/ directory with naming: system_<name>_v*_*.txt
#
#
# Version 2 prompts generate structured markdown with ## headers for each topic
# This enables individual topic selection when building prompt contexts
#
SYSTEM_PROMPT_SINGLE_CHUNK=format-single-chunk-v2
SYSTEM_PROMPT_MULTI_CHUNK=format-multi-chunk-v2

# Processing Configuration
# Control how transcriptions are chunked and processed
#
# CHUNK_SIZE: Maximum size in bytes for each transcription chunk
#   - Large files are split into chunks for processing at intelligent boundaries
#   - Splits occur at natural points: speaker changes, paragraphs, timestamps, sentences
#   - Default: 100000 (100KB)
#   - Increase for fewer API calls, decrease for more granular processing
#
CHUNK_SIZE=100000

# OVERLAP_SIZE: Context overlap between chunks for continuity
#   - Default: 1000 characters
#   - Provides surrounding context from adjacent chunks to maintain natural flow
#   - Used only for context during formatting, not included in final output
#   - Increase for better continuity, decrease to reduce token usage
#
OVERLAP_SIZE=1000

# REQUEST_SIZE_LIMIT: Maximum size for API request bodies
#   - Limits the size of files that can be uploaded/processed
#   - Default: 50mb
#   - Increase if you have very large transcription files
#
REQUEST_SIZE_LIMIT=50mb

# Claude API Token Limits
# Maximum tokens for Claude API responses
#
# MAX_TOKENS_TRANSCRIPTION: For automatic transcription formatting
#   - Default: 4096
#   - Each chunk gets this many tokens for formatting
#   - Increase if formatting is being cut off
#
MAX_TOKENS_TRANSCRIPTION=4096

# MAX_TOKENS_PROMPT: For user prompts and artifact generation
#   - Default: 8192
#   - Increase for longer, more detailed responses
#   - Note: Higher values = higher costs
#
MAX_TOKENS_PROMPT=8192

# DEBUG_WRITE_CHUNKS: Write original and formatted chunks to separate debug files
#   - Default: false
#   - Set to "true" to enable chunk debugging (writes debug_chunk_* files to transcriptions directory)
#   - Useful for diagnosing data loss or chunk processing issues
#
DEBUG_WRITE_CHUNKS=false

# LangSmith Tracing and Monitoring
# LangSmith provides observability for LLM applications, tracking requests, responses, and performance
#
# LANGSMITH_TRACING: Enable/disable LangSmith tracing
#   - Default: false (disabled)
#   - Set to "true" to enable tracing and monitoring in LangSmith
#   - When disabled, the application works normally without LangSmith
#   - Requires LANGSMITH_API_KEY to be set when enabled
#
LANGSMITH_TRACING=false

# LANGSMITH_API_KEY: Your LangSmith API key
#   - Required only if LANGSMITH_TRACING=true
#   - Get your API key from: https://smith.langchain.com/settings
#   - Leave blank or commented out if not using LangSmith
#
# LANGSMITH_API_KEY=lsv2_pt_your-api-key-here

# LANGSMITH_PROJECT: Project name in LangSmith dashboard
#   - Default: interview-transcription-manager
#   - Used to organize traces in LangSmith
#   - Only relevant when LANGSMITH_TRACING=true
#
LANGSMITH_PROJECT=interview-transcription-manager

# LANGSMITH_ENDPOINT: LangSmith API endpoint
#   - Default: https://api.smith.langchain.com
#   - Usually doesn't need to be changed
#   - Only relevant when LANGSMITH_TRACING=true
#
# LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# RAG (Retrieval-Augmented Generation) Configuration
# RAG enables semantic search across interview transcriptions to provide relevant context to agents
#
# RAG_ENABLED: Enable/disable RAG features
#   - Default: true
#   - Set to "false" to disable RAG functionality
#   - Uses in-memory vector store (no persistence between restarts)
#   - Requires ~100MB model download on first run
#
RAG_ENABLED=true

# RAG_TOP_K: Number of relevant chunks to retrieve per query
#   - Default: 3
#   - Higher values provide more context but may dilute relevance
#   - Recommended range: 3-7
#
RAG_TOP_K=3

# RAG_CHUNK_SIZE: Size of text chunks for embedding (in characters)
#   - Default: 1500
#   - Smaller chunks = more precise retrieval
#   - Larger chunks = more context per result
#
RAG_CHUNK_SIZE=1500

# RAG_CHUNK_OVERLAP: Overlap between consecutive chunks (in characters)
#   - Default: 150
#   - Helps maintain context across chunk boundaries
#   - Typically 10% of chunk size
#
RAG_CHUNK_OVERLAP=150

# RAG_AUTO_SYNC_ON_STARTUP: Automatically index formatted documents on server startup
#   - Default: true
#   - Set to "false" to require manual sync via UI button
#   - Only indexes interview_formatted_*.md files
#
RAG_AUTO_SYNC_ON_STARTUP=true

# RAG_EMBEDDING_MODEL: Embedding model for semantic search
#   - Default: Xenova/all-MiniLM-L6-v2
#   - Runs locally, no API calls required
#   - Model downloaded on first use (~100MB)
#   - Good balance of speed and quality
#
RAG_EMBEDDING_MODEL=Xenova/all-MiniLM-L6-v2
